{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "Model Creation.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euAWnuGW0HKv"
      },
      "source": [
        "# 1. Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGTwq4R60HK5"
      },
      "source": [
        "import random\n",
        "import copy\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import re\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer, HashingVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gensim\n",
        "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
        "import nltk\n",
        "#nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHZ5-EL70HK7"
      },
      "source": [
        "# 2. Read the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVaJYkUW0HK8"
      },
      "source": [
        "dialogues = pd.read_csv(\"data/dialogues.tsv\",sep=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBcsuet10HK9"
      },
      "source": [
        "posts = pd.read_csv(\"data/tagged_posts.tsv\",sep=\"\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnK4cV4_0HK9",
        "outputId": "6df0590d-b362-4e81-8622-a40b1dadcf19"
      },
      "source": [
        "dialogues.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Okay -- you're gonna need to learn how to lie.</td>\n",
              "      <td>dialogue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I'm kidding.  You know how sometimes you just ...</td>\n",
              "      <td>dialogue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Like my fear of wearing pastels?</td>\n",
              "      <td>dialogue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I figured you'd get to the good stuff eventually.</td>\n",
              "      <td>dialogue</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thank God!  If I had to hear one more story ab...</td>\n",
              "      <td>dialogue</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text       tag\n",
              "0     Okay -- you're gonna need to learn how to lie.  dialogue\n",
              "1  I'm kidding.  You know how sometimes you just ...  dialogue\n",
              "2                   Like my fear of wearing pastels?  dialogue\n",
              "3  I figured you'd get to the good stuff eventually.  dialogue\n",
              "4  Thank God!  If I had to hear one more story ab...  dialogue"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NHQ_woh0HK_",
        "outputId": "8c8b8035-dc00-45f2-b64d-c89a27f09515"
      },
      "source": [
        "posts.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>post_id</th>\n",
              "      <th>title</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9</td>\n",
              "      <td>Calculate age in C#</td>\n",
              "      <td>c#</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>16</td>\n",
              "      <td>Filling a DataSet or DataTable from a LINQ que...</td>\n",
              "      <td>c#</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>39</td>\n",
              "      <td>Reliable timer in a console application</td>\n",
              "      <td>c#</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>42</td>\n",
              "      <td>Best way to allow plugins for a PHP application</td>\n",
              "      <td>php</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>59</td>\n",
              "      <td>How do I get a distinct, ordered list of names...</td>\n",
              "      <td>c#</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   post_id                                              title  tag\n",
              "0        9                                Calculate age in C#   c#\n",
              "1       16  Filling a DataSet or DataTable from a LINQ que...   c#\n",
              "2       39            Reliable timer in a console application   c#\n",
              "3       42    Best way to allow plugins for a PHP application  php\n",
              "4       59  How do I get a distinct, ordered list of names...   c#"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncO1oGdk0HLA",
        "outputId": "ea6cab82-c1ea-4971-bc96-5e0997a918eb"
      },
      "source": [
        "print(\"Num Posts:\",len(posts))\n",
        "print(\"Num Dialogues:\",len(dialogues))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num Posts: 2171575\n",
            "Num Dialogues: 218609\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X4g784P0HLA"
      },
      "source": [
        "# 3. Create training data for intent classifier - Chitchat/SO Question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghPabty40HLB"
      },
      "source": [
        "texts  =  list(dialogues[:200000].text.values) + list(posts[:200000].title.values)\n",
        "labels =  ['dialogue']*200000 + ['stackoverflow']*200000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbY56lzd0HLB"
      },
      "source": [
        "data = pd.DataFrame({'text':texts,'target':labels})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI7BGIde0HLC"
      },
      "source": [
        "def text_prepare(text):\n",
        "    \"\"\"Performs tokenization and simple preprocessing.\"\"\"\n",
        "    \n",
        "    replace_by_space_re = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "    bad_symbols_re = re.compile('[^0-9a-z #+_]')\n",
        "    stopwords_set = set(stopwords.words('english'))\n",
        "\n",
        "    text = text.lower()\n",
        "    text = replace_by_space_re.sub(' ', text)\n",
        "    text = bad_symbols_re.sub('', text)\n",
        "    text = ' '.join([x for x in text.split() if x and x not in stopwords_set])\n",
        "\n",
        "    return text.strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eru_cYAS0HLD"
      },
      "source": [
        "# Doing some data cleaning\n",
        "data['text'] = data['text'].apply(lambda x : text_prepare(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ExB9mOc0HLD",
        "outputId": "8ed1637d-fda6-4582-fa5a-4255db357435"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data['text'],data['target'],test_size = .1 , random_state=0)\n",
        "\n",
        "print('Train size = {}, test size = {}'.format(len(X_train), len(X_test)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size = 360000, test size = 40000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GRBW1SS0HLE"
      },
      "source": [
        "# 4. Create Intent classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Aqj_19j0HLE"
      },
      "source": [
        "# We will keep our models and vectorizers in this folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTnpAqan0HLF",
        "outputId": "466b3509-df5e-41a0-ef04-cfca70f47b3f"
      },
      "source": [
        "!mkdir resources"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: resources: File exists\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iH656S-80HLF"
      },
      "source": [
        "def tfidf_features(X_train, X_test, vectorizer_path):\n",
        "    \"\"\"Performs TF-IDF transformation and dumps the model.\"\"\"\n",
        "    tfv = TfidfVectorizer(dtype=np.float32, min_df=3,  max_features=None, \n",
        "            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
        "            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
        "            stop_words = 'english')\n",
        "    \n",
        "    X_train = tfv.fit_transform(X_train)\n",
        "    X_test = tfv.transform(X_test)\n",
        "    \n",
        "    pickle.dump(tfv,vectorizer_path)\n",
        "    return X_train, X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i6r1Edr0HLG"
      },
      "source": [
        "X_train_tfidf, X_test_tfidf = tfidf_features(X_train, X_test, open(\"resources/tfidf.pkl\",'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srujZ6GI0HLG",
        "outputId": "68f45cd0-a5eb-4595-860d-12c1cb8c0714"
      },
      "source": [
        "intent_recognizer = LogisticRegression(C=10,random_state=0)\n",
        "intent_recognizer.fit(X_train_tfidf,y_train)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=0, solver='warn',\n",
              "          tol=0.0001, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utm0uEgZ0HLG",
        "outputId": "a556925c-5343-48ad-eac0-55c10b341302"
      },
      "source": [
        "# Check test accuracy.\n",
        "y_test_pred = intent_recognizer.predict(X_test_tfidf)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print('Test accuracy = {}'.format(test_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy = 0.989825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijC0dtqt0HLH"
      },
      "source": [
        "pickle.dump(intent_recognizer, open(\"resources/intent_clf.pkl\" , 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK2n6KiH0HLI"
      },
      "source": [
        "# 5 Create Programming Language classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH3kdKsl0HLI"
      },
      "source": [
        "X = posts['title'].values\n",
        "y = posts['tag'].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LZ8iEqX0HLJ",
        "outputId": "1ee39fc6-cf10-458d-9b87-6ce88b0b508f"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "print('Train size = {}, test size = {}'.format(len(X_train), len(X_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train size = 1737260, test size = 434315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtoJY0LP0HLJ"
      },
      "source": [
        "vectorizer = pickle.load(open(\"resources/tfidf.pkl\", 'rb'))\n",
        "X_train_tfidf, X_test_tfidf = vectorizer.transform(X_train), vectorizer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vn-FM4Y0HLK",
        "outputId": "5ceb5d36-1903-4907-f7dd-40ff23041f9e"
      },
      "source": [
        "tag_classifier = OneVsRestClassifier(LogisticRegression(C=5,random_state=0))\n",
        "tag_classifier.fit(X_train_tfidf,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/miniconda3/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OneVsRestClassifier(estimator=LogisticRegression(C=5, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
              "          n_jobs=None, penalty='l2', random_state=0, solver='warn',\n",
              "          tol=0.0001, verbose=0, warm_start=False),\n",
              "          n_jobs=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elFwxbEI0HLK",
        "outputId": "327fa68f-6563-4556-db9b-349e6a41c32a"
      },
      "source": [
        "# Check test accuracy.\n",
        "y_test_pred = tag_classifier.predict(X_test_tfidf)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "print('Test accuracy = {}'.format(test_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy = 0.8043816124241622\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMjgfQbX0HLL"
      },
      "source": [
        "pickle.dump(tag_classifier, open(\"resources/tag_clf.pkl\", 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMgWtqg90HLL"
      },
      "source": [
        "# 6. Store Question database Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9KQKqjP0HLL"
      },
      "source": [
        "You can use [pre-trained word vectors](https://code.google.com/archive/p/word2vec/) from Google."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgWd2aeu0HLM"
      },
      "source": [
        "\n",
        "\n",
        "# Load Google's pre-trained Word2Vec model.\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxSJCEQD0HLN"
      },
      "source": [
        "We want to convert every question to an embedding and store them. Whenever user asks a stack overflow question we want to use cosine similarity to get the most similar question"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aU3YGS8t0HLO"
      },
      "source": [
        "def question_to_vec(question, embeddings, dim=300):\n",
        "    \"\"\"\n",
        "        question: a string\n",
        "        embeddings: dict where the key is a word and a value is its' embedding\n",
        "        dim: size of the representation\n",
        "\n",
        "        result: vector representation for the question\n",
        "    \"\"\"\n",
        "    word_tokens = question.split(\" \")\n",
        "    question_len = len(word_tokens)\n",
        "    question_mat = np.zeros((question_len,dim), dtype = np.float32)\n",
        "    \n",
        "    for idx, word in enumerate(word_tokens):\n",
        "        if word in embeddings:\n",
        "            question_mat[idx,:] = embeddings[word]\n",
        "            \n",
        "    # remove zero-rows which stand for OOV words       \n",
        "    question_mat = question_mat[~np.all(question_mat == 0, axis = 1)]\n",
        "    \n",
        "    # Compute the mean of each word along the sentence\n",
        "    if question_mat.shape[0] > 0:\n",
        "        vec = np.array(np.mean(question_mat, axis = 0), dtype = np.float32).reshape((1,dim))\n",
        "    else:\n",
        "        vec = np.zeros((1,dim), dtype = np.float32)\n",
        "        \n",
        "    return vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lVgy6TC0HLP"
      },
      "source": [
        "counts_by_tag = posts.groupby(by=['tag'])[\"tag\"].count().reset_index(name = 'count').sort_values(['count'], ascending = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvifIprH0HLP",
        "outputId": "cfe238b5-973b-4bde-ed82-87d07e1f496f"
      },
      "source": [
        "counts_by_tag = list(zip(counts_by_tag['tag'],counts_by_tag['count']))\n",
        "print(counts_by_tag)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('c#', 394451), ('java', 383456), ('javascript', 375867), ('php', 321752), ('c_cpp', 281300), ('python', 208607), ('ruby', 99930), ('r', 36359), ('vb', 35044), ('swift', 34809)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUntIeCb0HLQ",
        "outputId": "1c32928c-d47c-482e-fb7a-2101759875f7"
      },
      "source": [
        "! mkdir resources/embeddings_folder"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: resources/embeddings_folder: File exists\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuRen63Z0HLQ"
      },
      "source": [
        "for tag, count in counts_by_tag:\n",
        "    tag_posts = posts[posts['tag'] == tag]\n",
        "    tag_post_ids = tag_posts['post_id'].values\n",
        "    tag_vectors = np.zeros((count, 300), dtype=np.float32)\n",
        "    for i, title in enumerate(tag_posts['title']):\n",
        "        tag_vectors[i, :] = question_to_vec(title, model, 300)\n",
        "    # Dump post ids and vectors to a file.\n",
        "    filename = 'resources/embeddings_folder/'+ tag + '.pkl'\n",
        "    pickle.dump((tag_post_ids, tag_vectors), open(filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKDfBaZX0HLR"
      },
      "source": [
        "# Given a question and tag can I retrieve the most similar question post_id\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOMsKy4G0HLS"
      },
      "source": [
        "\n",
        "def get_similar_question(question,tag):\n",
        "    # get the path where all question embeddings are kept and load the post_ids and post_embeddings\n",
        "    embeddings_path = 'resources/embeddings_folder/' + tag + \".pkl\"\n",
        "    post_ids, post_embeddings = pickle.load(open(embeddings_path, 'rb'))\n",
        "    # Get the embeddings for the question\n",
        "    question_vec = question_to_vec(question, model, 300)\n",
        "    # find index of most similar post\n",
        "    best_post_index = pairwise_distances_argmin(question_vec,\n",
        "                                                post_embeddings)\n",
        "    # return best post id\n",
        "    return post_ids[best_post_index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUtnh6oI0HLS",
        "outputId": "dafded36-4411-4ba9-d7f4-3cff4ce2ac12"
      },
      "source": [
        "get_similar_question(\"how to use list comprehension in python?\",'python')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5947137])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9iOADBv0HLT"
      },
      "source": [
        "You can find this question at:\n",
        "    \n",
        "https://stackoverflow.com/questions/8278287"
      ]
    }
  ]
}